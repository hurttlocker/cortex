# Cortex v0.9.0 Release Notes

Date: 2026-02-25
Tag: `v0.9.0`

## Summary

`v0.9.0` is the **LLM-Augmented Intelligence** release. Six features transform fact extraction from noisy regex patterns into a smart, LLM-powered pipeline. Enrichment is on by default â€” but everything works offline without any API keys.

## Highlights

### ðŸ§  Query Expansion (#216)
- `--expand` flag pre-processes vague queries through an LLM to generate better search terms
- Default model: Gemini 2.0 Flash (free tier)
- LRU cache prevents duplicate LLM calls
- Graceful fallback to original query on failure

### ðŸ”¬ LLM Enrichment (#218, #227)
- Grok 4.1 Fast finds facts that rules miss: decisions, relationships, implicit connections
- **On by default** when `--extract` is used. Pass `--no-enrich` for offline mode.
- Additive-only â€” never removes or modifies rule-extracted facts
- Tagged as `llm-enrich` for provenance tracking

### ðŸ·ï¸ Auto-Classification (#219, #227)
- DeepSeek V3.2 reclassifies generic `kv` facts into proper types (decision, config, state, temporal, identity, location, preference, relationship)
- Runs automatically at import time when enrichment is enabled
- `cortex classify` for bulk sweeps: batch-size 20, concurrency 5, live progress logging
- 77.6% reclassification rate, 0.12% error rate on 20K facts

### âš”ï¸ Conflict Auto-Resolution (#217)
- `cortex conflicts --resolve llm` analyzes contradictory facts and recommends actions
- Actions: supersede (mark loser), merge (combine), keep (both valid)
- Confidence gating at 0.7 â€” low-confidence resolutions flagged for human review
- Batch processing with configurable concurrency, dry-run support

### ðŸ“Š Fact Clustering & Summarization (#220)
- `cortex summarize` consolidates clusters of related facts into concise summaries
- Reduces fact count while preserving knowledge
- Uses LLM to synthesize overlapping facts

### ðŸ”§ Tightened Governor (#227)
- MaxFacts: 20â†’10 (default), 15â†’5 (auto-capture)
- MinPredicate: 4â†’5 chars, MinObject: 2â†’3 chars
- 6 new noise filters: section headers, bold subjects, file path predicates, long objects (>200 chars), checkbox items
- Rule-based kv output reduced â‰¥50%

## New Flags

| Flag | What it does |
|------|-------------|
| `--no-enrich` | Skip LLM enrichment (rule-only extraction) |
| `--no-classify` | Skip auto-classification at import time |
| `--expand` | Enable LLM query expansion on search |
| `--concurrency N` | Parallel LLM batch requests (default 5) |
| `--resolve llm` | LLM-powered conflict resolution |

## Graceful Degradation

No API key? No problem. Cortex detects missing keys and falls back to rule-only extraction with a friendly message:

```
Skipping LLM enrichment (no API key). Pass --no-enrich to silence this.
```

## Cost

All LLM features are optional. Typical ongoing cost: <$1/month.

| Feature | Model | Cost |
|---------|-------|------|
| Enrichment | Grok 4.1 Fast (OpenRouter) | ~$0.01/file |
| Classification | DeepSeek V3.2 (OpenRouter) | ~$0.50/20K facts |
| Query expansion | Gemini 2.0 Flash | Free |
| Conflict resolution | Any OpenRouter model | ~$0.01/batch |

## Upgrading

If you have an existing database with many generic `kv` facts:

```bash
# Option A: Fresh start (recommended)
cortex reimport ~/notes/ --recursive --extract --force

# Option B: Keep memories, fix fact types
cortex classify --limit 50000 --batch-size 20 --concurrency 5
```

## Validation

- 15 test packages, 300+ tests, all passing
- Classification benchmark: 20K facts in 53 min, 77.6% reclassification, 0.12% errors
- 6-model benchmark across enrichment + classification tasks
- Graceful degradation tested: no API key â†’ rules-only, no crash

## New Packages

- `internal/llm/` â€” LLM provider abstraction (Google AI + OpenRouter)
- `internal/search/expand.go` â€” Query expansion with LRU cache
- `internal/extract/enrich.go` â€” LLM-powered fact enrichment
- `internal/extract/classify.go` â€” Batch fact classification
- `internal/extract/resolve.go` â€” LLM conflict resolution
- `internal/extract/summarize.go` â€” Cluster consolidation
